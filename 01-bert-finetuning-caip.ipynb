{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2020 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHLV0D7Y5jtU"
   },
   "source": [
    "# BERT fine-tuning using AI Platform Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to use [AI Platform (Unified)](https://cloud.google.com/ai-platform-unified/docs/start/introduction-unified-platform) to run TensorFlow 2.x distributed training with GPUs. Both single node and multi-worker scenarios are covered.\n",
    "\n",
    "The ML scenario is BERT fine-tuning. You will use the text IMDB movie reviews database and the pre-trained BERT model from the [TensorFlow Hub](https://www.tensorflow.org/hub) to develop a text classification model for sentiment analysis.\n",
    "\n",
    "\n",
    "There are three types of AI Platform resources you can use to train custom models on AI Platform:\n",
    "\n",
    "- [Custom jobs](https://cloud.google.com/ai-platform-unified/docs/training/create-custom-job)\n",
    "- [Hyperparameter tuning jobs](https://cloud.google.com/ai-platform-unified/docs/training/using-hyperparameter-tuning)\n",
    "- [Training pipelines](https://cloud.google.com/ai-platform-unified/docs/training/create-training-pipeline)\n",
    "\n",
    "This sample focuses on [Custom jobs](https://cloud.google.com/ai-platform-unified/docs/training/create-custom-job) with [custom training containers](https://cloud.google.com/ai-platform-unified/docs/training/containers-overview).\n",
    "\n",
    "In the notebook, you will go through the following steps:\n",
    "\n",
    "- Converting the text IMDB database to the TFRecords format\n",
    "- Developing a custom training container\n",
    "- Configuring, submitting, and monitoring single node and multi-worker Custom training jobs\n",
    "\n",
    "\n",
    "### About BERT\n",
    "\n",
    "\n",
    "[BERT](https://arxiv.org/abs/1810.04805) and other Transformer encoder architectures have been wildly successful on a variety of tasks in NLP (natural language processing). They compute vector-space representations of natural language that are suitable for use in deep learning models. The BERT family of models uses the Transformer encoder architecture to process each token of input text in the full context of all tokens before and after, hence the name: Bidirectional Encoder Representations from Transformers. \n",
    "\n",
    "BERT models are usually pre-trained on a large corpus of text, then fine-tuned for specific tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "## Setting up the environment\n",
    "\n",
    "### Setting up your notebook environment\n",
    "\n",
    "This notebook has been tested with [AI Platform Notebooks](https://cloud.google.com/ai-platform/notebooks/docs) configured with the standard TensorFlow 2.4 image.\n",
    "\n",
    "It may work on other environments as long as the similar hardware and software configuration is used.\n",
    "\n",
    "#### Provisioning an instance of AI Platform Notebooks\n",
    "\n",
    "To provision an instance of AI Platform Notebooks, follow the instructions in the [AI Platform Notebooks documentation](https://cloud.google.com/ai-platform/notebooks/docs/create-new). Configure  your instance with multiple GPUs and use the TensorFlow 2.4 image. \n",
    "\n",
    "#### Installing software pre-requisities\n",
    "\n",
    "In addition to standard packages pre-installed in the TensorFlow 2.4 image you need the following additional packages:\n",
    "- [AI Platform Python client library](https://cloud.google.com/ai-platform-unified/docs/start/client-libraries), and\n",
    "- [TensorFlow Model Garden library](https://github.com/tensorflow/models/tree/master/official)\n",
    "- [TF.Text](https://www.tensorflow.org/tutorials/tensorflow_text/intro)\n",
    "\n",
    "Use `pip` to install the libraries. You can run `pip` from a terminal window of your AI Platform Notebooks instance or execute the following cells. Make sure to restart the notebook after installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U google-cloud-aiplatform --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tf-models-official tensorflow-text --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you installed the packages from within the notebookd make sure to restart the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "### Setting up your GCP project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a GCP project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "2. [Make sure that billing is enabled for your project.](https://cloud.google.com/billing/docs/how-to/modify-project)\n",
    "\n",
    "3. [Enable the AI Platform APIs, Compute Engine APIs and Container Registry API.](https://console.cloud.google.com/flows/enableapi?apiid=ml.googleapis.com,compute_component,containerregistry.googleapis.com)\n",
    "\n",
    "4. [Google Cloud SDK](https://cloud.google.com/sdk) is already installed in AI Platform Notebooks.\n",
    "\n",
    "5. Enter your project ID in the cell below. Then run the  cell to make sure the\n",
    "Cloud SDK uses the right project for all the commands in this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set your project ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oe_QtcD8xa3y"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = 'jk-demos'\n",
    "\n",
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4t4fNIrxa3z"
   },
   "source": [
    "#### Set the default region\n",
    "\n",
    "You can also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook.  Below are regions supported for AI Platform (Unified). We recommend when possible, to choose the region closest to you. \n",
    "\n",
    "- Americas: `us-central1`\n",
    "- Europe: `europe-west4`\n",
    "- Asia Pacific: `asia-east1`\n",
    "\n",
    "You can not use a Multi-Regional Storage bucket for training with AI Platform. Not all regions provide support for all AI Platform services. For the lastest support per region, see [Region support for AI Platform (Unified) services](https://cloud.google.com/ai-platform-unified/docs/general/locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YMA5Tb-8xa30"
   },
   "outputs": [],
   "source": [
    "REGION = 'us-central1' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "#### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "In this tutorial, your training job retrieves data and  saves the artifacts created during the job, including\n",
    "a trained model, checkpoints, and the TensorBoard logs, into a Google Cloud storage bucket. \n",
    "\n",
    "Set the name of your Cloud Storage bucket below. It must be unique across all\n",
    "Cloud Storage buckets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"jk-demos-bucket\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION gs://$BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucvCsknMCims"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhOb7YnwClBb"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al gs://$BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OxBoSThVxa34"
   },
   "source": [
    "#### Set AI Platform (Unified) constants\n",
    "\n",
    "Let's now setup some constants for AI Platform (Unified):\n",
    "\n",
    "- `API_ENDPOINT`: The AI Platform (Unified) API service endpoint for dataset, model, job, pipeline and endpoint services.\n",
    "- `API_PREDICT_ENDPOINT`: The AI Platform (Unified) API service endpoint for prediction.\n",
    "- `PARENT`: The AI Platform (Unified) location root path for dataset, model and endpoint resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZtcw173xa34"
   },
   "outputs": [],
   "source": [
    "# API Endpoint\n",
    "API_ENDPOINT = \"{}-aiplatform.googleapis.com\".format(REGION)\n",
    "API_PREDICT_ENDPOINT = \"{}-prediction-aiplatform.googleapis.com\".format(REGION)\n",
    "\n",
    "# AI Platform (Unified) location root path for your dataset, model and endpoint resources\n",
    "PARENT = \"projects/\" + PROJECT_ID + \"/locations/\" + REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "#### Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value\n",
    "\n",
    "from google.cloud.aiplatform import gapic as aip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6ppE7imft-y"
   },
   "source": [
    "### Preparing data\n",
    "\n",
    "In this section, you will convert the original IMDB dataset that is in plain text into the [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) format. The TFRecord format is recommended for high performance input pipelines that are critical in large scale training scenarios like BERT pre-training and fine-tuning. The TFRecord format works well with the [tf.data API](https://www.tensorflow.org/guide/data) used to implement input pipelines in this sample.\n",
    "\n",
    "After the TFRecord files are created, you will copy them to a GCS storage bucket. In most distributed training scenarios, training data needs to be located in a shared storage location.\n",
    "\n",
    "#### Download the IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dir = os.path.expanduser('~')\n",
    "local_dir = f'{local_dir}/datasets'\n",
    "\n",
    "if tf.io.gfile.exists(local_dir):\n",
    "    tf.io.gfile.rmtree(local_dir)\n",
    "tf.io.gfile.makedirs(local_dir)\n",
    "\n",
    "url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "local_path = f'{local_dir}/aclImdb_v1.tar.gz'\n",
    "\n",
    "dataset = tf.keras.utils.get_file(local_path, url,\n",
    "                                  untar=True, \n",
    "                                  cache_dir=local_dir,\n",
    "                                  cache_subdir='.'\n",
    "                                  )\n",
    "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "\n",
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "\n",
    "# remove unused folders to make it easier to load the data\n",
    "remove_dir = os.path.join(train_dir, 'unsup')\n",
    "shutil.rmtree(remove_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the IMDB dataset to TFRecords files\n",
    "\n",
    "##### Create training, validation and testing splits from IMDB text files\n",
    "\n",
    "The IMDB dataset has already been divided into train and test, but it lacks a validation set. We will create a validation set using an 80:20 split of the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_splits(train_dir, test_dir, val_split, seed):\n",
    "    \n",
    "    train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "        train_dir,\n",
    "        validation_split=val_split,\n",
    "        subset='training',\n",
    "        seed=seed)\n",
    "\n",
    "    class_names = train_ds.class_names\n",
    "    \n",
    "    train_ds = train_ds.unbatch()\n",
    "\n",
    "    val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "        train_dir,\n",
    "        validation_split=val_split,\n",
    "        subset='validation',\n",
    "        seed=seed).unbatch()\n",
    "\n",
    "    test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "        test_dir).unbatch()\n",
    "\n",
    "    return train_ds, val_ds, test_ds, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "val_split = 0.2\n",
    "test_dir = f'{dataset_dir}/test'\n",
    "\n",
    "train_ds, val_ds, test_ds, class_names = (\n",
    "    create_splits(train_dir, test_dir, val_split, seed)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inspect a couple of examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text, label in train_ds.take(2):\n",
    "    print(f'Review: {text.numpy()}')\n",
    "    label = label.numpy()\n",
    "    print(f'Label : {label} ({class_names[label]})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare tf.Example serialization routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_example(text_fragment, label):\n",
    "    \"\"\"Serializes text fragment and label in tf.Example.\"\"\"\n",
    "    \n",
    "    def _bytes_feature(value):\n",
    "        \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "        if isinstance(value, type(tf.constant(0))):\n",
    "            value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    def _int64_feature(value):\n",
    "        \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "    \n",
    "    feature = {\n",
    "        'text_fragment': _bytes_feature(text_fragment),\n",
    "        'label': _int64_feature(label)\n",
    "    }\n",
    "    \n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()\n",
    "    \n",
    "def tf_serialize_example(text_fragment, label):\n",
    "  tf_string = tf.py_function(\n",
    "    serialize_example,\n",
    "    (text_fragment, label), \n",
    "    tf.string)      \n",
    "  return tf.reshape(tf_string, ()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Write TFRecords files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrecords_folder = '{}/tfrecords'.format(os.path.expanduser('~'))\n",
    "if tf.io.gfile.exists(tfrecords_folder):\n",
    "    tf.io.gfile.rmtree(tfrecords_folder)\n",
    "tf.io.gfile.makedirs(tfrecords_folder)\n",
    "\n",
    "filenames = ['train.tfrecords', 'valid.tfrecords', 'test.tfrecords']\n",
    "for file_name, dataset in zip(filenames, [train_ds, val_ds, test_ds]):\n",
    "    writer = tf.data.experimental.TFRecordWriter(os.path.join(tfrecords_folder, file_name))\n",
    "    writer.write(dataset.map(tf_serialize_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Double check that you can read the created TFRecord files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in tf.data.TFRecordDataset([os.path.join(tfrecords_folder, file_name)]).take(2):\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copy the created TFRecord files to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_paths = [f'gs://{BUCKET_NAME}/tfrecords/train',\n",
    "             f'gs://{BUCKET_NAME}/tfrecords/valid',\n",
    "             f'gs://{BUCKET_NAME}/tfrecords/test']\n",
    "\n",
    "for filename, gcs_path in zip(filenames, gcs_paths):\n",
    "    local_file_path = os.path.join(tfrecords_folder, filename)\n",
    "    gcs_file_path = f'{gcs_path}/{filename}'\n",
    "    !gsutil cp {local_file_path} {gcs_file_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the training container image\n",
    "\n",
    "\n",
    "There are two ways of packaging your training code for AI Platform Custom jobs. \n",
    "\n",
    "- **Use a Google Cloud prebuilt container**. If you use a prebuilt container, you will additionally specify a Python package to install into the container image. This Python package contains your code for training a custom model.\n",
    "\n",
    "- **Use your own custom container image**. If you use your own container, the container needs to contain your code for training a custom model.\n",
    "\n",
    "In this sampel, we are using a custom container.\n",
    "\n",
    "To create a custom training container you need to define a Python training module and package it in a container image together with all the required dependencies.\n",
    "\n",
    "We will use the standard [Deep Learning Containers](https://cloud.google.com/ai-platform/deep-learning-containers/docs) image as a base image for the custom traininer container image. Specifically we are going to use the `gcr.io/deeplearning-platform-release/tf2-gpu.2-4` image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BASE_IMAGE = 'gcr.io/deeplearning-platform-release/tf2-gpu.2-4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fn-LlmWGxa35"
   },
   "source": [
    "\n",
    "### Create the training module\n",
    "\n",
    "A custom training image encapsulates you training code. You can structure your code in anyway you want as long as you can invoke it through a standard docker container interface. \n",
    "\n",
    "In this sample, the training code is encapsulated in a single Python module - `task.py`.  The below section summarizes key design decisions taken when designing the training regime.\n",
    "\n",
    "#### Model design\n",
    "\n",
    "This sample implements a simple classification model using pre-trained BERT components from TensorFlow Hub. Specifically a classic BERT architecture with L=12 hidden layers, a hidden size of H=768, and A=12 attention heads is used. This [TF Hub model](https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3) uses the implementatio of BERT from the [TensorFlow Model Garden repository](https://github.com/tensorflow/models/tree/master/official/nlp/bert). \n",
    "\n",
    "Text inputs need to be transformed to numeric token ids and arranged in several Tensors before being input to BERT. TensorFlow Hub provides a matching preprocessing model for each of the BERT models, which implements this transformation using TF ops from the TF.text library. It is not necessary to run pure Python code outside your TensorFlow model to preprocess text.\n",
    "\n",
    "The model implemented in the script embedds the preprocessing model from TF Hub as a Keras layer.\n",
    "\n",
    "Since this is a binary classification problem and the model outputs a probability (a single-unit layer), the model uses `tf.keras.losses.BinaryCrossentropy` loss function and `tf.metrics.BinaryAccuracy` metric.\n",
    "\n",
    "The sample uses the same optimizer that BERT was originally trained with: the \"Adaptive Moments\" (Adam). This optimizer minimizes the prediction loss and does regularization by weight decay (not using moments), which is also known as [AdamW](https://arxiv.org/abs/1711.05101).\n",
    "\n",
    "For the learning rate (`init_lr`), the same schedule as BERT pre-training is used: linear decay of a notional initial learning rate, prefixed with a linear warm-up phase over the first 10% of training steps (`num_warmup_steps`). In line with the BERT paper, the initial learning rate is smaller for fine-tuning (best of 5e-5, 3e-5, 2e-5).\n",
    "\n",
    "#### Input pipelines\n",
    "\n",
    "The training code utilizes `tf.data` to implement input pipelines. The common techniques for optimizing performance - caching, prefetching - are applied. To better support distributed training the script allows for explicit configuration of [Auto Sharding Policy](https://www.tensorflow.org/tutorials/distribute/input).\n",
    "\n",
    "#### Fault tolerance\n",
    "\n",
    "The script utilizes the [`tf.keras.callbacks.experimental.BackupAndRestore`](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#backupandrestore_callback) callback for resilience from failures during training. The callback provides fault tolerance, by backing up the model and current epoch number in a temporary checkpoint file. This is done at the endo of each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R4yEjEt8xa36"
   },
   "outputs": [],
   "source": [
    "! rm -rf trainer\n",
    "! mkdir trainer\n",
    "! touch trainer/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile trainer/task.py\n",
    "\n",
    "\n",
    "# Copyright 2021 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "from official.nlp import optimization \n",
    "\n",
    "\n",
    "TFHUB_HANDLE_ENCODER = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3'\n",
    "TFHUB_HANDLE_PREPROCESS = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
    "LOCAL_TB_FOLDER = '/tmp/logs'\n",
    "LOCAL_SAVED_MODEL_DIR = '/tmp/saved_model'\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_integer('steps_per_epoch', 625, 'Steps per training epoch')\n",
    "flags.DEFINE_integer('eval_steps', 150, 'Evaluation steps')\n",
    "flags.DEFINE_integer('epochs', 2, 'Nubmer of epochs')\n",
    "flags.DEFINE_integer('per_replica_batch_size', 32, 'Per replica batch size')\n",
    "flags.DEFINE_string('training_data_path', 'gs://jk-demos-bucket/tfrecords/train', 'Training data GCS path')\n",
    "flags.DEFINE_string('validation_data_path', 'gs://jk-demos-bucket/tfrecords/valid', 'Validation data GCS path')\n",
    "flags.DEFINE_string('testing_data_path', 'gs://jk-demos-bucket/data/imdb/test', 'Testing data GCS path')\n",
    "\n",
    "flags.DEFINE_string('job_dir', 'gs://jk-demos-bucket/jobs', 'A base GCS path for jobs')\n",
    "flags.DEFINE_enum('strategy', 'multiworker', ['mirrored', 'multiworker'], 'Distribution strategy')\n",
    "flags.DEFINE_enum('auto_shard_policy', 'auto', ['auto', 'data', 'file', 'off'], 'Dataset sharing strategy')\n",
    "\n",
    "\n",
    "\n",
    "auto_shard_policy = {\n",
    "    'auto': tf.data.experimental.AutoShardPolicy.AUTO,\n",
    "    'data': tf.data.experimental.AutoShardPolicy.DATA,\n",
    "    'file': tf.data.experimental.AutoShardPolicy.FILE,\n",
    "    'off': tf.data.experimental.AutoShardPolicy.OFF,\n",
    "}\n",
    "\n",
    "\n",
    "def create_unbatched_dataset(tfrecords_folder):\n",
    "    \"\"\"Creates an unbatched dataset in the format required by the \n",
    "       sentiment analysis model from the folder with TFrecords files.\"\"\"\n",
    "    \n",
    "    feature_description = {\n",
    "        'text_fragment': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "        'label': tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    }\n",
    "\n",
    "    def _parse_function(example_proto):\n",
    "        parsed_example = tf.io.parse_single_example(example_proto, feature_description)\n",
    "        return parsed_example['text_fragment'], parsed_example['label']\n",
    "  \n",
    "    file_paths = [f'{tfrecords_folder}/{file_path}' for file_path in tf.io.gfile.listdir(tfrecords_folder)]\n",
    "    dataset = tf.data.TFRecordDataset(file_paths)\n",
    "    dataset = dataset.map(_parse_function)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def configure_dataset(ds, auto_shard_policy):\n",
    "    \"\"\"\n",
    "    Optimizes the performance of a dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = (\n",
    "        auto_shard_policy\n",
    "    )\n",
    "    \n",
    "    ds = ds.repeat(-1).cache()\n",
    "    ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    ds = ds.with_options(options)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def create_input_pipelines(train_dir, valid_dir, test_dir, batch_size, auto_shard_policy):\n",
    "    \"\"\"Creates input pipelines from Imdb dataset.\"\"\"\n",
    "    \n",
    "    train_ds = create_unbatched_dataset(train_dir)\n",
    "    train_ds = train_ds.batch(batch_size)\n",
    "    train_ds = configure_dataset(train_ds, auto_shard_policy)\n",
    "    \n",
    "    valid_ds = create_unbatched_dataset(valid_dir)\n",
    "    valid_ds = valid_ds.batch(batch_size)\n",
    "    valid_ds = configure_dataset(valid_ds, auto_shard_policy)\n",
    "    \n",
    "    test_ds = create_unbatched_dataset(test_dir)\n",
    "    test_ds = test_ds.batch(batch_size)\n",
    "    test_ds = configure_dataset(test_ds, auto_shard_policy)\n",
    "\n",
    "    return train_ds, valid_ds, test_ds\n",
    "\n",
    "\n",
    "def build_classifier_model(tfhub_handle_preprocess, tfhub_handle_encoder):\n",
    "    \"\"\"Builds a simple binary classification model with BERT trunk.\"\"\"\n",
    "    \n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "    encoder_inputs = preprocessing_layer(text_input)\n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    net = outputs['pooled_output']\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
    "    \n",
    "    return tf.keras.Model(text_input, net)\n",
    "\n",
    "\n",
    "def copy_tensorboard_logs(local_path: str, gcs_path: str):\n",
    "    \"\"\"Copies Tensorboard logs from a local dir to a GCS location.\n",
    "    \n",
    "    After training, batch copy Tensorboard logs locally to a GCS location. This can result\n",
    "    in faster pipeline runtimes over streaming logs per batch to GCS that can get bottlenecked\n",
    "    when streaming large volumes.\n",
    "    \n",
    "    Args:\n",
    "      local_path: local filesystem directory uri.\n",
    "      gcs_path: cloud filesystem directory uri.\n",
    "    Returns:\n",
    "      None.\n",
    "    \"\"\"\n",
    "    pattern = '{}/*/events.out.tfevents.*'.format(local_path)\n",
    "    local_files = tf.io.gfile.glob(pattern)\n",
    "    gcs_log_files = [local_file.replace(local_path, gcs_path) for local_file in local_files]\n",
    "    for local_file, gcs_file in zip(local_files, gcs_log_files):\n",
    "        tf.io.gfile.copy(local_file, gcs_file)\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "    del argv\n",
    "    \n",
    "    def _is_chief(task_type, task_id):\n",
    "        return ((task_type == 'chief' or task_type == 'worker') and task_id == 0) or task_type is None\n",
    "        \n",
    "    \n",
    "    logging.info('Setting up training.')\n",
    "    logging.info('   epochs: {}'.format(FLAGS.epochs))\n",
    "    logging.info('   steps_per_epoch: {}'.format(FLAGS.steps_per_epoch))\n",
    "    logging.info('   eval_steps: {}'.format(FLAGS.eval_steps))\n",
    "    logging.info('   strategy: {}'.format(FLAGS.strategy))\n",
    "    \n",
    "    if FLAGS.strategy == 'mirrored':\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "    else:\n",
    "        strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "        \n",
    "    if strategy.cluster_resolver:    \n",
    "        task_type, task_id = (strategy.cluster_resolver.task_type,\n",
    "                              strategy.cluster_resolver.task_id)\n",
    "    else:\n",
    "        task_type, task_id =(None, None)\n",
    "        \n",
    "    \n",
    "    global_batch_size = (strategy.num_replicas_in_sync *\n",
    "                         FLAGS.per_replica_batch_size)\n",
    "    \n",
    "    \n",
    "    train_ds, valid_ds, test_ds = create_input_pipelines(\n",
    "        FLAGS.training_data_path,\n",
    "        FLAGS.validation_data_path,\n",
    "        FLAGS.testing_data_path,\n",
    "        global_batch_size,\n",
    "        auto_shard_policy[FLAGS.auto_shard_policy])\n",
    "        \n",
    "    num_train_steps = FLAGS.steps_per_epoch * FLAGS.epochs\n",
    "    num_warmup_steps = int(0.1*num_train_steps)\n",
    "    init_lr = 3e-5\n",
    "    \n",
    "    with strategy.scope():\n",
    "        model = build_classifier_model(TFHUB_HANDLE_PREPROCESS, TFHUB_HANDLE_ENCODER)\n",
    "        loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        metrics = tf.metrics.BinaryAccuracy()\n",
    "        optimizer = optimization.create_optimizer(\n",
    "            init_lr=init_lr,\n",
    "            num_train_steps=num_train_steps,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            optimizer_type='adamw')\n",
    "\n",
    "        model.compile(optimizer=optimizer,\n",
    "                      loss=loss,\n",
    "                      metrics=metrics)\n",
    "        \n",
    "    # Configure BackupAndRestore callback\n",
    "    backup_dir = '{}/backupandrestore'.format(FLAGS.job_dir)\n",
    "    callbacks = [tf.keras.callbacks.experimental.BackupAndRestore(backup_dir=backup_dir)]\n",
    "    \n",
    "    # Configure TensorBoard callback on Chief\n",
    "    if _is_chief(task_type, task_id):\n",
    "        callbacks.append(tf.keras.callbacks.TensorBoard(\n",
    "            log_dir=LOCAL_TB_FOLDER, update_freq='batch'))\n",
    "    \n",
    "    logging.info('Starting training ...')\n",
    "    \n",
    "    history = model.fit(x=train_ds,\n",
    "                        validation_data=valid_ds,\n",
    "                        steps_per_epoch=FLAGS.steps_per_epoch,\n",
    "                        validation_steps=FLAGS.eval_steps,\n",
    "                        epochs=FLAGS.epochs,\n",
    "                        callbacks=callbacks)\n",
    "\n",
    "    if _is_chief(task_type, task_id):\n",
    "        # Copy tensorboard logs to GCS\n",
    "        tb_logs = '{}/tb_logs'.format(FLAGS.job_dir)\n",
    "        logging.info('Copying TensorBoard logs to: {}'.format(tb_logs))\n",
    "        copy_tensorboard_logs(LOCAL_TB_FOLDER, tb_logs)\n",
    "        saved_model_dir = '{}/saved_model'.format(FLAGS.job_dir)\n",
    "    else:\n",
    "        saved_model_dir = LOCAL_SAVED_MODEL_DIR\n",
    "        \n",
    "    # Save trained model\n",
    "    saved_model_dir = '{}/saved_model'.format(FLAGS.job_dir)\n",
    "    logging.info('Training completed. Saving the trained model to: {}'.format(saved_model_dir))\n",
    "    model.save(saved_model_dir)\n",
    "    #tf.saved_model.save(model, saved_model_dir)\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    logging.set_verbosity(logging.INFO)\n",
    "    app.run(main)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a docker file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMAGE = f'gcr.io/{PROJECT_ID}/imdb_bert'\n",
    "\n",
    "dockerfile = f'''\n",
    "FROM {TRAIN_BASE_IMAGE}\n",
    "\n",
    "RUN pip install pip install tf-models-official tensorflow-text \n",
    "\n",
    "WORKDIR /\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY trainer /trainer\n",
    "\n",
    "# Sets up the entry point to invoke the trainer.\n",
    "ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]\n",
    "'''\n",
    "\n",
    "with open('Dockerfile', 'w') as f:\n",
    "    f.write(dockerfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a container image and upload it to your Container Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker build -t {TRAIN_IMAGE} ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker push {TRAIN_IMAGE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively you use Cloud Build."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!gcloud builds submit --tag {TRAIN_IMAGE} ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the image locally\n",
    "\n",
    "It may be difficult to troubleshoot distributed training jobs running in AI Platform. You can perform some level of troubleshooting by simulating a distributed training environment on your AI Platform Notebooks instance.\n",
    "\n",
    "Let's assume that you have provisioned your instance with 4 GPUs. To simulate a distributed environment with two nodes, each equipped with two GPUs you can start two local containers configured as per below sample commands. Execute these commands from Jupyter terminal windows.\n",
    "\n",
    "```\n",
    "docker run --rm -it --gpus '\"device=0,1\"' \\\n",
    "--env TF_CONFIG='{\"cluster\": {\"worker\": [\"localhost:12345\", \"localhost:23456\"]}, \"task\": {\"type\": \"worker\", \"index\": 0} }' \\\n",
    "--network=host \\\n",
    "gcr.io/jk-demos/imdb_bert --epochs=2 --steps_per_epoch=20 --eval_steps=10 --auto_shard_policy=data --job_dir=gs://jk-demos-bucket/test_run\n",
    "```\n",
    "\n",
    "```\n",
    "docker run --rm -it --gpus '\"device=2,3\"' \\\n",
    "--env TF_CONFIG='{\"cluster\": {\"worker\": [\"localhost:12345\", \"localhost:23456\"]}, \"task\": {\"type\": \"worker\", \"index\": 1} }' \\\n",
    "--network=host \\\n",
    "gcr.io/jk-demos/imdb_bert --epochs=2 --steps_per_epoch=20 --eval_steps=10 --auto_shard_policy=data --job_dir=gs://jk-demos-bucket/test_run\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting training jobs\n",
    "\n",
    "The AI Platform (Unified) SDK works as a client/server model. On your side, the Python script, you  create a client that sends requests and receives responses from the server -- AI Platform. Requests and responses conform to the schemas documented in [AI Platform API Reference](https://cloud.google.com/ai-platform-unified/docs/reference/rest/v1/projects.locations.batchPredictionJobs/create).\n",
    "\n",
    "We will use the term specification to refer to a formatted request. To submit a Custom job request you need to create a Custom job specification.\n",
    "\n",
    "The custom job specification comprises two parts:\n",
    "- A worker pool configuration, and\n",
    "- A scheduling configuration\n",
    "\n",
    "For single-node training, you define a single worker pool. For multi-node distributed training, multiple worker pools are defined.\n",
    "\n",
    "Within a worker pool specification, you configure:\n",
    "- Machine types and accelerators\n",
    "- Configuration of what training code the worker pool runs. \n",
    "\n",
    "For jobs using custom containers (like in this sample), the latter section of a worker pool specification contains a custom container configuration, including the URI of the container image and parameters passed to the container.\n",
    "\n",
    "The scheduling configuration includes parameters related to queuing and scheduling of custom jobs, including the maximum job running time and the job restart policy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assembling the job specification for single-node training\n",
    "\n",
    "For this job we will use a single `n1-standard-4` machine with 2 NVidia V100 GPUs and the container image created in the previous sections of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MACHINE_TYPE = 'n1-standard-4'\n",
    "TRAIN_GPU, TRAIN_NGPU = (aip.AcceleratorType.NVIDIA_TESLA_V100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When configuring a custom container you pass the command line parameters expected by your script through the `args` field of the container specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "steps_per_epoch = 200\n",
    "eval_steps = 50\n",
    "training_data_path = 'gs://jk-demos-bucket/tfrecords/train'\n",
    "validation_data_path = 'gs://jk-demos-bucket/tfrecords/valid'\n",
    "job_id = 'job-{}'.format(datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "job_dir = 'gs://jk-demos-bucket/jobs/{}'.format(job_id)\n",
    "\n",
    "worker_pool_spec = [\n",
    "    {\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": MACHINE_TYPE,\n",
    "            \"accelerator_type\": TRAIN_GPU,\n",
    "            \"accelerator_count\": TRAIN_NGPU\n",
    "        },\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": TRAIN_IMAGE,\n",
    "            \"args\": [\n",
    "                \"--epochs=\" + str(epochs),\n",
    "                \"--steps_per_epoch=\" + str(steps_per_epoch),\n",
    "                \"--eval_steps=\" + str(eval_steps),\n",
    "                \"--training_data_path=\" + training_data_path,\n",
    "                \"--validation_data_path=\" + validation_data_path,\n",
    "                \"--job_dir=\" + job_dir,\n",
    "                \"--strategy=mirrored\",\n",
    "                \"--auto_shard_policy=data\",\n",
    "            ]\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "custom_job = {\n",
    "        \"display_name\": f'imdb-bert-{job_id}',\n",
    "        \"job_spec\": {\n",
    "            \"worker_pool_specs\": worker_pool_spec\n",
    "        },\n",
    "    }\n",
    "\n",
    "custom_job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting the job\n",
    "\n",
    "To submit the job you need to create a Job Service client and invoke the `create_custom_job` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_options = {\"api_endpoint\": API_ENDPOINT}\n",
    "\n",
    "client = aip.JobServiceClient(client_options=client_options)\n",
    "response = client.create_custom_job(parent=PARENT, custom_job=custom_job)\n",
    "job_name = response.name\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitoring the job\n",
    "\n",
    "You can monitor the job through GCP Console or programmaticaly by using the `client.get_custom_job()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.get_custom_job(name=job_name)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assembling the job specification for multi-worker training\n",
    "\n",
    "If you run a distributed training job with AI Platform, you specify multiple machines (nodes) in a training cluster. The training service allocates the resources for the machine types you specify. Your running job on a given node is called a replica.\n",
    "\n",
    "Each replica in the training cluster is given a single role or task in distributed training. For example:\n",
    "\n",
    "- Primary replica: Exactly one replica is designated the primary replica. This task manages the others and reports status for the job as a whole.\n",
    "- Worker(s): One or more replicas may be designated as workers. These replicas do their portion of the work as you designate in your job configuration.\n",
    "- Parameter server(s): If supported by your ML framework, one or more replicas may be designated as parameter servers. These replicas store model parameters and coordinate shared model state between the workers.\n",
    "- Evaluator(s): If supported by your ML framework, one or more replicas may be designated as evaluators. These replicas can be used to evaluate your model. If you are using TensorFlow, note that TensorFlow generally expects that you use no more than one evaluator.\n",
    "\n",
    "You configure the role by mapping to a worker pool specification:\n",
    "\n",
    "- First worker pool specification (index 0 in the `workerPoolSpecs` list) maps to Primary or chief worker. There can be only one replica configured in the first worker pool specification\n",
    "- Second worker pool specification maps to secondary workers\n",
    "- Third worker pool specification maps to parameters servers, and\n",
    "- Fourth worker pool specification maps to evaluators\n",
    "\n",
    "Our second job will be a multi-worker distributed training job with one chief and one secondary worker. Both replicas will run on `n1-standard-4` machines with two NVidia V100 GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MACHINE_TYPE = 'n1-standard-4'\n",
    "TRAIN_GPU, TRAIN_NGPU = (aip.AcceleratorType.NVIDIA_TESLA_V100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When configuring a custom container you pass the command line parameters expected by your script through the `args` field of the container specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "steps_per_epoch = 200\n",
    "eval_steps = 50\n",
    "training_data_path = 'gs://jk-demos-bucket/tfrecords/train'\n",
    "validation_data_path = 'gs://jk-demos-bucket/tfrecords/valid'\n",
    "job_id = 'job-{}'.format(datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "job_dir = 'gs://jk-demos-bucket/jobs/{}'.format(job_id)\n",
    "\n",
    "worker_pool_spec = [\n",
    "    {\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": MACHINE_TYPE,\n",
    "            \"accelerator_type\": TRAIN_GPU,\n",
    "            \"accelerator_count\": TRAIN_NGPU\n",
    "        },\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": TRAIN_IMAGE,\n",
    "            \"args\": [\n",
    "                \"--epochs=\" + str(epochs),\n",
    "                \"--steps_per_epoch=\" + str(steps_per_epoch),\n",
    "                \"--eval_steps=\" + str(eval_steps),\n",
    "                \"--training_data_path=\" + training_data_path,\n",
    "                \"--validation_data_path=\" + validation_data_path,\n",
    "                \"--job_dir=\" + job_dir,\n",
    "                \"--strategy=multiworker\",\n",
    "                \"--auto_shard_policy=data\",\n",
    "            ]\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": MACHINE_TYPE,\n",
    "            \"accelerator_type\": TRAIN_GPU,\n",
    "            \"accelerator_count\": TRAIN_NGPU\n",
    "        },\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": TRAIN_IMAGE,\n",
    "            \"args\": [\n",
    "                \"--epochs=\" + str(epochs),\n",
    "                \"--steps_per_epoch=\" + str(steps_per_epoch),\n",
    "                \"--eval_steps=\" + str(eval_steps),\n",
    "                \"--training_data_path=\" + training_data_path,\n",
    "                \"--validation_data_path=\" + validation_data_path,\n",
    "                \"--job_dir=\" + job_dir,\n",
    "                \"--strategy=multiworker\",\n",
    "                \"--auto_shard_policy=data\",\n",
    "            ]\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "custom_job = {\n",
    "        \"display_name\": f'imdb-bert-{job_id}',\n",
    "        \"job_spec\": {\n",
    "            \"worker_pool_specs\": worker_pool_spec\n",
    "        },\n",
    "    }\n",
    "\n",
    "custom_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_options = {\"api_endpoint\": API_ENDPOINT}\n",
    "\n",
    "client = aip.JobServiceClient(client_options=client_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.create_custom_job(parent=PARENT, custom_job=custom_job)\n",
    "job_name = response.name\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = response.name\n",
    "\n",
    "response = client.get_custom_job(name=job_name)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ucaip_customjob_image_container.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-gpu.2-4.mnightly-2021-02-12-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:mnightly-2021-02-12-debian-10-test"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
